{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da362cb",
   "metadata": {},
   "source": [
    "This notebook can be run in Google Colab by clicking the \"Open in Colab\" button below and setting `colab = True` in the \"Setup\" code block below.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/frithureiks/Computational-Detection-of-Syllable-Boundaries/blob/main/NeuralNet_model/Paper_Experiments_from_Saved_Models.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae98fb1",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "If running in Google Colab, set `colab = True` and this will download the repository and dataset into Colab's file structure and set the appropriate file paths.\n",
    "\n",
    "If running locally, ensure you have already cloned the GitHub repository and downloaded the data from Zenodo, then set the corresponding file paths to the folders containing each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03698f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "colab = True\n",
    "\n",
    "if colab == True:\n",
    "\n",
    "    # Clone this repository into Colab\n",
    "    !git clone https://github.com/frithureiks/Computational-Detection-of-Syllable-Boundaries.git\n",
    "\n",
    "    # Download annotated data\n",
    "    !pip install zenodo_get\n",
    "    !zenodo_get https://doi.org/10.5281/zenodo.17418250\n",
    "    !unzip '/content/Data_and_Models.zip' -d '/content'\n",
    "\n",
    "    # Set paths to folders containing code and data\n",
    "    code_folder = '/content/Computational-Detection-of-Syllable-Boundaries'\n",
    "    data_folder = '/content/Data'\n",
    "    results_folder = '/content'\n",
    "    model_folder ='/content/Model_Weights'\n",
    "\n",
    "else:\n",
    "    # Set paths to folders containing code and data\n",
    "    code_folder = 'Path/To/Local/Cloned/Github/Repository'\n",
    "    data_folder = 'Path/To/Downloaded/Data/Folder'\n",
    "    results_folder = 'Path/To/Folder/To/Save/Figures'\n",
    "    model_folder = 'Path/To/Downloaded/Models/Folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11371b2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.chdir(code_folder)\n",
    "from NeuralNet_model import NgramCNN, WordDataset, EqualLengthsBatchSampler, format_k, syllabify, match_syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2255a",
   "metadata": {},
   "source": [
    "# Hyperparameters and Random Seed\n",
    "\n",
    "We set the batch size (64), epochs (10), and random seed (100) to those used in the paper. The model will load onto GPU if available, otherwise CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "seed = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dc111",
   "metadata": {},
   "source": [
    "# Languages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583eb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['cs','nl','en','fr','de','el','it','ko','no','es','sv','tr']\n",
    "\n",
    "names = ['Czech','Dutch','English','French','German','Greek',\n",
    "         'Italian','Korean','Norwegian','Spanish','Swedish','Turkish']\n",
    "\n",
    "with open(os.path.join(data_folder, 'Word_Dataset_Train.pickle'), 'rb') as f:\n",
    "    train_ds = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_folder, 'Word_Dataset_Val.pickle'), 'rb') as f:\n",
    "    val_ds = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_folder, 'Word_Dataset_Test.pickle'), 'rb') as f:\n",
    "    test_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b6bf4",
   "metadata": {},
   "source": [
    "# Load CNN\n",
    "\n",
    "The following code block loads the trained CNN files for each language into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "\n",
    "for lang, name in zip(languages, names):\n",
    "    model = NgramCNN(n_gram=7, n_filters=15, n_layers=10).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_folder, f'LOO_{name}_CNN.pt'),\n",
    "                                              map_location=device, weights_only=True))\n",
    "    models[lang] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb67cf7",
   "metadata": {},
   "source": [
    "# Run Predictions on Test Set\n",
    "\n",
    "This runs the saved models on the test set and saves their predictions. Alternatively, you can load the previously saved predictions in the following code block, which should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47629428",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = dict()\n",
    "loo_outputs = []\n",
    "\n",
    "for lang in languages:\n",
    "    # Select model\n",
    "    model = models[lang]\n",
    "\n",
    "    # Set Test Dataset and Dataloader\n",
    "    word_idx, loo_test_ds = zip(*[(i,entry) for i,entry in enumerate(test_ds) if entry[2] == lang])\n",
    "    test_dl = DataLoader(loo_test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Run predictions on Test Dataloader\n",
    "    probs, truths = model.predict(test_dl)\n",
    "    ids = [[id]*len(entry[0]) for id, entry in zip(word_idx, loo_test_ds)]\n",
    "    ids = [id for id_list in ids for id in id_list]\n",
    "    positions = [i+1 for entry in loo_test_ds for i in range(len(entry[0]))]\n",
    "    segments = [segment for entry in loo_test_ds for segment in entry[3]]\n",
    "    surprisals = [surprisal for entry in loo_test_ds for surprisal in entry[0]]\n",
    "\n",
    "    outputs[lang] = pd.DataFrame({'Probabilities':probs, 'Ground Truths':truths, 'Languages':lang, 'Word Indices':ids,\n",
    "                                  'Positions':positions, 'Segments':segments, 'Surprisals':surprisals})\n",
    "\n",
    "    # Load prediction thresholds\n",
    "    with open(os.path.join(data_folder, 'Prediction_Thresholds.json')) as f:\n",
    "        model_thresholds = json.load(f)\n",
    "    threshold = model_thresholds['CNN'][lang]\n",
    "\n",
    "    # Add threshold and binary predictions to output\n",
    "    outputs[lang]['Predictions'] = (outputs[lang]['Probabilities'] >= threshold).astype(float)\n",
    "    outputs[lang]['Thresholds'] = threshold\n",
    "\n",
    "    outputs[lang] = outputs[lang][['Word Indices','Positions','Segments','Predictions','Ground Truths',\n",
    "                                   'Probabilities','Thresholds','Surprisals','Languages']]\n",
    "    loo_outputs.append(outputs[lang])\n",
    "\n",
    "loo_outputs = pd.concat(loo_outputs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4062863",
   "metadata": {},
   "source": [
    "# Load Test Set Predictions\n",
    "\n",
    "Loads previously saved model predictions on the test set. If you ran the previous code block to generate new predictions, running this block will overwrite them, though the results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "loo_outputs = pd.read_csv(os.path.join(data_folder, 'LOO_Outputs.csv'))\n",
    "\n",
    "# Originally positions were 0-indexed, but to calculate metrics below they must be 1-indexed\n",
    "loo_outputs['Positions'] = loo_outputs['Positions'] + 1\n",
    "\n",
    "# Additional formatting\n",
    "loo_outputs = loo_outputs[loo_outputs['Model']=='CNN']\n",
    "loo_outputs = loo_outputs.rename(columns={'Codes':'Languages'})\n",
    "loo_outputs[['Word Indices','Positions']] = loo_outputs[['Word Indices','Positions']].astype(int)\n",
    "loo_outputs = loo_outputs[['Word Indices','Positions','Segments','Predictions','Ground Truths',\n",
    "                           'Probabilities','Thresholds','Surprisals','Languages']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d8a23",
   "metadata": {},
   "source": [
    "# Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=seed)\n",
    "random_results = loo_outputs.copy()\n",
    "random_results['Predictions'] = rng.integers(0,2,len(random_results))\n",
    "idx = random_results['Positions']==1\n",
    "random_results.loc[idx, 'Predictions'] = 1\n",
    "random_nohead = random_results[random_results['Positions']!=1]   # Remove first position from random_results\n",
    "\n",
    "random_metrics = dict()\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    random_nohead['Ground Truths'],random_nohead['Predictions'],average='binary')\n",
    "random_metrics['Precision'] = precision\n",
    "random_metrics['Recall'] = recall\n",
    "random_metrics['F1'] = f1\n",
    "\n",
    "counts = random_results['Positions'].value_counts()\n",
    "positions = counts.index\n",
    "segments = counts.values\n",
    "sylls, pred_sylls, entropy = [],[],[]\n",
    "precision, recall, f1, mcc = [],[],[],[]\n",
    "for pos in positions:\n",
    "    pos_df = random_results[random_results['Positions']==pos]\n",
    "    sylls.append(pos_df['Ground Truths'].sum())\n",
    "    pred_sylls.append(pos_df['Predictions'].sum())\n",
    "    entropy.append(pos_df['Surprisals'].mean())\n",
    "    p, r, f, _ = precision_recall_fscore_support(pos_df['Ground Truths'], pos_df['Predictions'], average='binary', zero_division=0)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "    f1.append(f)\n",
    "\n",
    "random_pos_metrics = pd.DataFrame([entropy, segments, sylls, pred_sylls, precision, recall, f1],\n",
    "                    index=['Entropy', 'Segments', 'Syllables', 'Predicted', 'Precision', 'Recall', 'F1'],\n",
    "                    columns=positions+1)\n",
    "random_pos_metrics = random_pos_metrics.T.astype({'Segments':int, 'Syllables':int, 'Predicted':int})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e4e16",
   "metadata": {},
   "source": [
    "# Segment Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_metrics = dict()\n",
    "df = loo_outputs.copy()\n",
    "df = df[df['Positions']!=1]         # Remove first position from segment_metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(df['Ground Truths'],df['Predictions'],average='binary')\n",
    "mcc = matthews_corrcoef(df['Ground Truths'],df['Predictions'])\n",
    "segment_metrics['Precision'] = precision\n",
    "segment_metrics['Recall'] = recall\n",
    "segment_metrics['F1'] = f1\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(3.5,3.5))\n",
    "x = np.arange(len(segment_metrics))\n",
    "bars = ax.bar(x, segment_metrics.values(), width=0.8, color='tab:blue', edgecolor='black', label='CNN')\n",
    "for m, metric in enumerate(random_metrics.values()):\n",
    "  ax.plot([m-0.36, m+0.37], [metric, metric], color='tab:orange', linewidth=4, label='Random',\n",
    "          path_effects=[pe.Stroke(linewidth=6, foreground='black'), pe.Normal()])\n",
    "ax.bar_label(bars, fmt='{:.0%}', padding=10)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xticks(x, segment_metrics.keys())\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[:1:-1], labels[:1:-1])\n",
    "plt.savefig(os.path.join(results_folder, 'CNN - Segment Metrics.pdf'), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867f013",
   "metadata": {},
   "source": [
    "# Segment Metrics by Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loo_outputs.copy()\n",
    "counts = df['Positions'].value_counts()\n",
    "positions = counts.index\n",
    "segments = counts.values\n",
    "frequency = segments / segments[0]\n",
    "sylls = [df[df['Positions']==pos]['Ground Truths'].sum() for pos in positions]\n",
    "pred_sylls = [df[df['Positions']==pos]['Predictions'].sum() for pos in positions]\n",
    "entropy = [df[df['Positions']==pos]['Surprisals'].mean() for pos in positions]\n",
    "\n",
    "metrics = [precision_recall_fscore_support(\n",
    "    df[df['Positions']==pos]['Ground Truths'], df[df['Positions']==pos]['Predictions'], average='binary', zero_division=0\n",
    "    ) for pos in positions]\n",
    "precision, recall, f1, _ = list(zip(*metrics))\n",
    "\n",
    "loo_pos_metrics = pd.DataFrame([entropy, segments, sylls, pred_sylls, precision, recall, f1, frequency],\n",
    "                    index=['Entropy', 'Segments', 'Syllables', 'Predicted', 'Precision', 'Recall', 'F1', 'Freq'],\n",
    "                    columns=positions)\n",
    "loo_pos_metrics = loo_pos_metrics.T.astype({'Segments':int, 'Syllables':int, 'Predicted':int})\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9,3), sharex=True)\n",
    "df = loo_pos_metrics\n",
    "positions = df.index\n",
    "precision = df['Precision']\n",
    "recall = df['Recall']\n",
    "rand_df = random_pos_metrics\n",
    "frequencies = df['Freq']\n",
    "\n",
    "k_formatter = ticker.FuncFormatter(format_k)\n",
    "ax[0].bar(positions, df['Segments'], width=1, color='tab:blue', edgecolor='black')\n",
    "ax[0].set_ylabel('Num Segments')\n",
    "ax[0].yaxis.set_major_formatter(k_formatter)\n",
    "ax[0].set_xlabel('Position')\n",
    "\n",
    "for j, (name, metric) in enumerate([('Precision',precision), ('Recall',recall)]):\n",
    "  j += 1\n",
    "  ax[j].bar(positions, metric, color='white', edgecolor='black', width=1)\n",
    "  bars = ax[j].bar(positions, metric, color='tab:blue', edgecolor='black', width=1)\n",
    "  for k, bar in enumerate(bars):\n",
    "    bar.set_alpha(df.loc[k+1,'Freq'])\n",
    "  ax[j].scatter(positions, rand_df[name], s=25, color='white', edgecolor='black')\n",
    "  points = ax[j].scatter(positions, rand_df[name], s=25, color=[(255/255,127/255,14/255,freq) for freq in frequencies],\n",
    "                         edgecolor='black', label='Random')\n",
    "  ax[j].set_xlabel('Position')\n",
    "  ax[j].set_ylabel(name)\n",
    "  ax[j].set_xticks(ticks=positions, labels=[str(i) if i % 5 == 0 else '' for i in positions])\n",
    "  ax[j].set_xlim(positions[0]-1, positions[-1]+1)\n",
    "  ax[j].legend(loc='upper center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_folder, f'CNN - Segment Metrics by Position.pdf'), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a550b91",
   "metadata": {},
   "source": [
    "# Predicted Syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054add76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loo_outputs.copy()\n",
    "df['Pred Segments'] = df.apply(syllabify, syllables='Predictions', segments='Segments', axis=1)\n",
    "df['True Segments'] = df.apply(syllabify, syllables='Ground Truths', segments='Segments', axis=1)\n",
    "word_groups = df.groupby(by=['Word Indices'])\n",
    "loo_words = pd.DataFrame()\n",
    "loo_words['Word'] = word_groups['Segments'].apply(lambda x: ''.join(x))\n",
    "loo_words['Languages'] = word_groups['Languages'].first()\n",
    "loo_words['True Syllables'] = word_groups['True Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "loo_words['Pred Syllables'] = word_groups['Pred Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "loo_words['Ground Truths'] = word_groups['Ground Truths'].apply(list)\n",
    "loo_words['Predictions'] = word_groups['Predictions'].apply(list)\n",
    "loo_words['True Matches'] = loo_words.apply(match_syllables, sylls='Ground Truths', ref_sylls='Predictions', axis=1)\n",
    "loo_words['Pred Matches'] = loo_words.apply(match_syllables, sylls='Predictions', ref_sylls='Ground Truths', axis=1)\n",
    "loo_words = loo_words[['Word', 'Languages', 'True Syllables', 'Pred Syllables', 'True Matches', 'Pred Matches']]\n",
    "\n",
    "preds = [pred for row in loo_words['Pred Syllables'] for pred in row]\n",
    "pred_len = [len(pred) for pred in preds]\n",
    "pred_pos = [i+1 for row in loo_words['Pred Syllables'] for i,syll in enumerate(row)]\n",
    "pred_ids = [[idx]*len(loo_words.loc[idx,'Pred Syllables']) for idx in loo_words.index]\n",
    "pred_ids = [id for idx in pred_ids for id in idx]\n",
    "pred_langs = [[loo_words.loc[idx, 'Languages']]*len(loo_words.loc[idx,'Pred Syllables']) for idx in loo_words.index]\n",
    "pred_langs = [lang for entry in pred_langs for lang in entry]\n",
    "pred_match = [pred for row in loo_words['Pred Matches'] for pred in row]\n",
    "loo_preds = pd.DataFrame({'Positions':pred_pos,'Syllables':preds,'Matches':pred_match, 'Lengths':pred_len,\n",
    "                          'Word Indices':pred_ids, 'Languages':pred_langs}).set_index('Positions')\n",
    "\n",
    "truths = [true for row in loo_words['True Syllables'] for true in row]\n",
    "true_len = [len(true) for true in truths]\n",
    "true_pos = [i+1 for row in loo_words['True Syllables'] for i,syll in enumerate(row)]\n",
    "true_ids = [[idx]*len(loo_words.loc[idx,'True Syllables']) for idx in loo_words.index]\n",
    "true_ids = [id for idx in true_ids for id in idx]\n",
    "true_langs = [[loo_words.loc[idx, 'Languages']]*len(loo_words.loc[idx,'True Syllables']) for idx in loo_words.index]\n",
    "true_langs = [lang for entry in true_langs for lang in entry]\n",
    "true_match = [true for row in loo_words['True Matches'] for true in row]\n",
    "loo_trues = pd.DataFrame({'Positions':true_pos,'Syllables':truths,'Matches':true_match, 'Lengths':true_len,\n",
    "                          'Word Indices':true_ids, 'Languages':true_langs}).set_index('Positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109760db",
   "metadata": {},
   "source": [
    "# Random Syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = random_results.copy()\n",
    "df['Pred Segments'] = df.apply(syllabify, syllables='Predictions', segments='Segments', axis=1)\n",
    "df['True Segments'] = df.apply(syllabify, syllables='Ground Truths', segments='Segments', axis=1)\n",
    "word_groups = df.groupby(by=['Word Indices'])\n",
    "random_words = pd.DataFrame()\n",
    "random_words['Word'] = word_groups['Segments'].apply(lambda x: ''.join(x))\n",
    "random_words['Languages'] = word_groups['Languages'].first()\n",
    "random_words['True Syllables'] = word_groups['True Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "random_words['Pred Syllables'] = word_groups['Pred Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "random_words['Ground Truths'] = word_groups['Ground Truths'].apply(list)\n",
    "random_words['Predictions'] = word_groups['Predictions'].apply(list)\n",
    "random_words['True Matches'] = random_words.apply(match_syllables, sylls='Ground Truths', ref_sylls='Predictions', axis=1)\n",
    "random_words['Pred Matches'] = random_words.apply(match_syllables, sylls='Predictions', ref_sylls='Ground Truths', axis=1)\n",
    "random_words = random_words[['Word', 'Languages', 'True Syllables', 'Pred Syllables', 'True Matches', 'Pred Matches']]\n",
    "\n",
    "preds = [pred for row in random_words['Pred Syllables'] for pred in row]\n",
    "pred_len = [len(pred) for pred in preds]\n",
    "pred_pos = [i+1 for row in random_words['Pred Syllables'] for i,syll in enumerate(row)]\n",
    "pred_ids = [[idx]*len(random_words.loc[idx,'Pred Syllables']) for idx in random_words.index]\n",
    "pred_ids = [id for idx in pred_ids for id in idx]\n",
    "pred_langs = [[random_words.loc[idx, 'Languages']]*len(random_words.loc[idx,'Pred Syllables']) for idx in random_words.index]\n",
    "pred_langs = [lang for entry in pred_langs for lang in entry]\n",
    "pred_match = [pred for row in random_words['Pred Matches'] for pred in row]\n",
    "random_preds = pd.DataFrame({'Positions':pred_pos,'Syllables':preds,'Matches':pred_match, 'Lengths':pred_len,\n",
    "                             'Word Indices':pred_ids, 'Languages':pred_langs}).set_index('Positions')\n",
    "\n",
    "truths = [true for row in random_words['True Syllables'] for true in row]\n",
    "true_len = [len(true) for true in truths]\n",
    "true_pos = [i+1 for row in random_words['True Syllables'] for i,syll in enumerate(row)]\n",
    "true_ids = [[idx]*len(random_words.loc[idx,'True Syllables']) for idx in random_words.index]\n",
    "true_ids = [id for idx in true_ids for id in idx]\n",
    "true_langs = [[random_words.loc[idx, 'Languages']]*len(random_words.loc[idx,'True Syllables']) for idx in random_words.index]\n",
    "true_langs = [lang for entry in true_langs for lang in entry]\n",
    "true_match = [true for row in random_words['True Matches'] for true in row]\n",
    "random_trues = pd.DataFrame({'Positions':true_pos,'Syllables':truths,'Matches':true_match, 'Lengths':true_len,\n",
    "                             'Word Indices':true_ids, 'Languages':true_langs}).set_index('Positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a003f0c",
   "metadata": {},
   "source": [
    "# Syllable Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "true_list = []\n",
    "for pos in loo_preds.index.unique():\n",
    "    pred_df = loo_preds.loc[[pos],['Syllables','Lengths','Matches','Languages']].reset_index()\n",
    "    pred_df['Count'] = pred_df['Syllables']\n",
    "    pred_df = pred_df.groupby(by=['Languages','Syllables'],as_index=False).agg(\n",
    "        {'Positions':'max', 'Lengths':'max', 'Matches':'sum', 'Count':'count'})\n",
    "    pred_df['True'] = pred_df['Matches'] > 0\n",
    "    pred_df = pred_df.set_index('Positions')\n",
    "    pred_list.append(pred_df)\n",
    "\n",
    "for pos in loo_trues.index.unique():\n",
    "    true_df = loo_trues.loc[[pos],['Syllables','Lengths','Matches','Languages']].reset_index()\n",
    "    true_df['Count'] = true_df['Syllables']\n",
    "    true_df = true_df.groupby(by=['Languages','Syllables'],as_index=False).agg(\n",
    "        {'Positions':'max', 'Lengths':'max', 'Matches':'sum', 'Count':'count'})\n",
    "    true_df['Predicted'] = true_df['Matches'] > 0\n",
    "    true_df = true_df.set_index('Positions')\n",
    "    true_list.append(true_df)\n",
    "\n",
    "loo_unique_preds = pd.concat(pred_list)\n",
    "loo_unique_trues = pd.concat(true_list)\n",
    "\n",
    "pred_list = []\n",
    "true_list = []\n",
    "for pos in random_preds.index.unique():\n",
    "    pred_df = random_preds.loc[[pos],['Syllables','Lengths','Matches','Languages']].reset_index()\n",
    "    pred_df['Count'] = pred_df['Syllables']\n",
    "    pred_df = pred_df.groupby(by=['Languages','Syllables'],as_index=False).agg(\n",
    "        {'Positions':'max', 'Lengths':'max', 'Matches':'sum', 'Count':'count'})\n",
    "    pred_df['True'] = pred_df['Matches'] > 0\n",
    "    #pred_df = pred_df[pred_df['Count']>10]\n",
    "    pred_df = pred_df.set_index('Positions')\n",
    "    pred_list.append(pred_df)\n",
    "\n",
    "for pos in random_trues.index.unique():\n",
    "    true_df = random_trues.loc[[pos],['Syllables','Lengths','Matches','Languages']].reset_index()\n",
    "    true_df['Count'] = true_df['Syllables']\n",
    "    true_df = true_df.groupby(by=['Languages','Syllables'],as_index=False).agg(\n",
    "        {'Positions':'max', 'Lengths':'max', 'Matches':'sum', 'Count':'count'})\n",
    "    true_df['Predicted'] = true_df['Matches'] > 0\n",
    "    true_df = true_df.set_index('Positions')\n",
    "    true_list.append(true_df)\n",
    "\n",
    "random_unique_preds = pd.concat(pred_list)\n",
    "random_unique_trues = pd.concat(true_list)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3.5,3.5))\n",
    "precision = loo_unique_preds['True'].mean()\n",
    "recall = loo_unique_trues['Predicted'].mean()\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "rand_prec = random_unique_preds['True'].mean()\n",
    "rand_recall = random_unique_trues['Predicted'].mean()\n",
    "rand_f1 = 2 * rand_prec * rand_recall / (rand_prec + rand_recall)\n",
    "x = [0,1,2]\n",
    "bars = ax.bar(x, [precision, recall, f1], color='tab:blue', edgecolor='black', label='CNN')\n",
    "for i, metric in enumerate([rand_prec, rand_recall, rand_f1]):\n",
    "    ax.plot([x[i]-0.36,x[i]+0.37],[metric, metric], color='tab:orange', linewidth=4, label='Random',\n",
    "            path_effects=[pe.Stroke(linewidth=6, foreground='black'), pe.Normal()])\n",
    "ax.bar_label(bars, fmt='{:.0%}', padding=3)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xticks(x, ['Precision', 'Recall', 'F1'])\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[:1:-1], labels[:1:-1])\n",
    "plt.savefig(os.path.join(results_folder, 'CNN - Unique Syllable Metrics.pdf'), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e49b4",
   "metadata": {},
   "source": [
    "# Syllable Metrics by Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pos = 0\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8.5,3))\n",
    "preds = loo_unique_preds\n",
    "trues = loo_unique_trues\n",
    "pred_pos = preds.index.unique()\n",
    "true_pos = trues.index.unique()\n",
    "positions = np.array(range(1, max(pred_pos.max(),true_pos.max())+1))\n",
    "if positions.max() > max_pos:\n",
    "    max_pos = positions.max()\n",
    "\n",
    "pred_all = [len(preds[preds.index==pos]) for pos in positions]\n",
    "true_all = [len(trues[trues.index==pos]) for pos in positions]\n",
    "pred_max = max(pred_all)\n",
    "true_max = max(true_all)\n",
    "\n",
    "precision = np.array([preds[preds.index==pos]['True'].mean() for pos in positions])\n",
    "recall = np.array([trues[trues.index==pos]['Predicted'].mean() for pos in positions])\n",
    "\n",
    "ax[0].bar(positions[:11]-0.2, pred_all[:11], width=0.4, color='tab:blue', edgecolor='black', label='Pred Syllables')\n",
    "ax[0].bar(positions[:11]+0.2, true_all[:11], width=0.4, color='tab:orange', edgecolor='black', label='True Syllables')\n",
    "ax[0].set_ylabel('Unique Syllables')\n",
    "ax[0].yaxis.set_major_formatter(k_formatter)\n",
    "ax[0].set_xticks(ticks=range(12), labels=['','','2','','4','','6','','8','','10',''])\n",
    "ax[0].set_xlabel('Position')\n",
    "\n",
    "rand_prec = np.array([random_unique_preds[random_unique_preds.index==pos]['True'].mean() for pos in positions])\n",
    "rand_recall = np.array([random_unique_trues[random_unique_trues.index==pos]['Predicted'].mean() for pos in positions])\n",
    "\n",
    "ax[1].bar(positions, precision, color='white', edgecolor='black', width=1)\n",
    "bars = ax[1].bar(positions, precision, color='tab:blue', edgecolor='black', width=1)\n",
    "for k, bar in enumerate(bars):\n",
    "    bar.set_alpha(pred_all[k]/pred_max)\n",
    "ax[1].scatter(positions, rand_prec, s=36, color='white', edgecolor='black')\n",
    "ax[1].scatter(positions, rand_prec, s=36, color=[(255/255,127/255,14/255,pred/pred_max) for pred in pred_all],\n",
    "              edgecolor='black', label='Random')\n",
    "ax[1].set_xticks(ticks=np.arange(1,max_pos+1), labels=[i if i%5==0 else '' for i in range(1,max_pos+1)])\n",
    "ax[1].set_yticks(ticks=np.arange(0,1.1,0.1), labels=[f'{i/10:.1f}' if i%2==0 else '' for i in range(0,11)])\n",
    "ax[1].set_xlabel('Position')\n",
    "ax[1].set_ylabel('Precision')\n",
    "ax[1].set_xlim(0,22)\n",
    "ax[1].legend(loc='upper left')\n",
    "\n",
    "ax[2].bar(positions, recall, color='white', edgecolor='black', width=1)\n",
    "bars = ax[2].bar(positions, recall, color='tab:blue', edgecolor='black', width=1)\n",
    "for k, bar in enumerate(bars):\n",
    "    bar.set_alpha(pred_all[k]/pred_max)\n",
    "ax[2].scatter(positions, rand_recall, s=36, color='white', edgecolor='black')\n",
    "ax[2].scatter(positions, rand_recall, s=36, color=[(255/255,127/255,14/255,true/true_max) for true in true_all],\n",
    "              edgecolor='black', label='Random')\n",
    "ax[2].set_xticks(ticks=np.arange(1,max_pos+1), labels=[i if i%5==0 else '' for i in range(1,max_pos+1)])\n",
    "ax[2].set_yticks(ticks=np.arange(0,1.1,0.1), labels=[f'{i/10:.1f}' if i%2==0 else '' for i in range(0,11)])\n",
    "ax[2].set_xlabel('Position')\n",
    "ax[2].set_ylabel('Recall')\n",
    "ax[2].set_xlim(0,12)\n",
    "ax[2].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_folder, 'CNN - Unique Syllable Metrics by Position.pdf'), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36057b48",
   "metadata": {},
   "source": [
    "# UDHR Common First Syllables\n",
    "\n",
    "Runs the CNN model trained on all languages except English on the English text of the Universal Declaration of Human Rights, and saves all beginning syllables predicted > 4 times. Alternatively, you can run the second block to load the presaved results, which should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UDHR Data\n",
    "udhr_surprisals = pd.read_csv(os.path.join(data_folder, 'UDHR_Dataset.csv'))\n",
    "udhr_ds = WordDataset(udhr_surprisals)\n",
    "udhr_dl = DataLoader(udhr_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# Predict Syllable Breaks\n",
    "probs, truths = models['en'].predict(udhr_dl)\n",
    "ids = [[i+1]*len(entry[0]) for i, entry in enumerate(udhr_ds)]\n",
    "ids = [id for id_list in ids for id in id_list]\n",
    "positions = [i+1 for entry in udhr_ds for i in range(len(entry[0]))]\n",
    "segments = [segment for entry in udhr_ds for segment in entry[3]]\n",
    "surprisals = [surprisal for entry in udhr_ds for surprisal in entry[0]]\n",
    "\n",
    "udhr_outputs = pd.DataFrame({'Probabilities':probs, 'Ground Truths':truths, 'Languages':'en', 'Word Indices':ids,\n",
    "                              'Positions':positions, 'Segments':segments, 'Surprisals':surprisals})\n",
    "\n",
    "threshold = model_thresholds['CNN'][lang]\n",
    "udhr_outputs['Predictions'] = (udhr_outputs['Probabilities'] >= threshold).astype(float)\n",
    "udhr_outputs['Thresholds'] = threshold\n",
    "\n",
    "udhr_outputs = udhr_outputs[['Word Indices','Positions','Segments','Predictions','Ground Truths',\n",
    "                              'Probabilities','Thresholds','Surprisals','Languages']]\n",
    "\n",
    "# Extract Syllables\n",
    "df = udhr_outputs.copy()\n",
    "df['Pred Segments'] = df.apply(syllabify, syllables='Predictions', segments='Segments', axis=1)\n",
    "df['True Segments'] = df.apply(syllabify, syllables='Ground Truths', segments='Segments', axis=1)\n",
    "word_groups = df.groupby(by=['Word Indices'])\n",
    "udhr_words = pd.DataFrame()\n",
    "udhr_words['Word'] = word_groups['Segments'].apply(lambda x: ''.join(x))\n",
    "udhr_words['Languages'] = word_groups['Languages'].first()\n",
    "udhr_words['True Syllables'] = word_groups['True Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "udhr_words['Pred Syllables'] = word_groups['Pred Segments'].apply(lambda x: ''.join(x).split('^'))\n",
    "udhr_words['Ground Truths'] = word_groups['Ground Truths'].apply(list)\n",
    "udhr_words['Predictions'] = word_groups['Predictions'].apply(list)\n",
    "udhr_words['True Matches'] = udhr_words.apply(match_syllables, sylls='Ground Truths', ref_sylls='Predictions', axis=1)\n",
    "udhr_words['Pred Matches'] = udhr_words.apply(match_syllables, sylls='Predictions', ref_sylls='Ground Truths', axis=1)\n",
    "udhr_words = udhr_words[['Word', 'Languages', 'True Syllables', 'Pred Syllables', 'True Matches', 'Pred Matches']]\n",
    "\n",
    "preds = [pred for row in udhr_words['Pred Syllables'] for pred in row]\n",
    "pred_len = [len(pred) for pred in preds]\n",
    "pred_pos = [i+1 for row in udhr_words['Pred Syllables'] for i,syll in enumerate(row)]\n",
    "pred_ids = [[idx]*len(udhr_words.loc[idx,'Pred Syllables']) for idx in udhr_words.index]\n",
    "pred_ids = [id for idx in pred_ids for id in idx]\n",
    "pred_langs = [[udhr_words.loc[idx, 'Languages']]*len(udhr_words.loc[idx,'Pred Syllables']) for idx in udhr_words.index]\n",
    "pred_langs = [lang for entry in pred_langs for lang in entry]\n",
    "pred_match = [pred for row in udhr_words['Pred Matches'] for pred in row]\n",
    "udhr_preds = pd.DataFrame({'Positions':pred_pos,'Syllables':preds,'Matches':pred_match, 'Lengths':pred_len,\n",
    "                          'Word Indices':pred_ids, 'Languages':pred_langs}).set_index('Positions')\n",
    "\n",
    "# Display beginning syllables predicted more than 4 times by the model\n",
    "udhr_syllables = udhr_preds.loc[[1],['Syllables','Lengths','Matches']].reset_index()\n",
    "udhr_syllables['Count'] = udhr_syllables['Syllables']\n",
    "udhr_syllables = udhr_syllables.groupby(by='Syllables',as_index=True).agg(\n",
    "    {'Lengths':'max', 'Matches':'mean', 'Count':'count'})\n",
    "udhr_syllables['True'] = udhr_syllables['Matches'] > 0\n",
    "udhr_syllables = udhr_syllables[udhr_syllables['Count']>4].reset_index()\n",
    "\n",
    "display(udhr_syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182631b7",
   "metadata": {},
   "source": [
    "# Load UDHR Predicted Syllables\n",
    "\n",
    "Load the saved predictions on the Universal Declaration of Human Rights. Should be identical to the above results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv(os.path.join(data_folder, 'UDHR_Predicted_Syllables.csv'), index_col=0)\n",
    "display(pred_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
